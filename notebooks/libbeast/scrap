static TensorNest array_pb_to_nest(kdbmultienv::NDArray* array_pb) {
    std::vector<int64_t> shape = {1, 1};  // [T=1, B=1].
    for (int i = 0, length = array_pb->shape_size(); i < length; ++i) {
      shape.push_back(array_pb->shape(i));
    }
    std::string* data = array_pb->release_data();
    at::ScalarType dtype = torch::utils::numpy_dtype_to_aten(array_pb->dtype());

    return TensorNest(torch::from_blob(
        data->data(), shape,
        /*deleter=*/[data](void*) { delete data; }, dtype));
  }

  static TensorNest multi_step_to_nest(kdbmultienv::MultiStep* multi_step) {
    std::vector<TensorNes<void>> multi_step;
    for () { // TODO make faster
      TensorNest done = TensorNest( // TODO check
        torch::full(
          {1, 1}, 
          multi_step->done(), 
          torch::dtype(torch::kBool)
        ));

      TensorNest reward = TensorNest(
        torch::full(
          {1, 1}, 
          multi_step->reward()
        ));

      TensorNest episode_step = TensorNest(
        torch::full(
          {1, 1}, 
          multi_step->episode_step(), 
          torch::dtype(torch::kInt32)
        ));

      TensorNest episode_return =
          TensorNest(
            torch::full(
              {1, 1}, 
              multi_step->episode_return()
            ));

      multi_step.push_back(TensorNest(std::vector(
          {
            nest_pb_to_nest(multi_step->mutable_observation(), array_pb_to_nest),
            std::move(reward), 
            std::move(done), 
            std::move(episode_step),
            std::move(episode_return)
          })));
    };
    return multi_step;
    
  }

  // TODO delve
  static void fill_ndarray_pb(kdbmultienv::NDArray* array,
                              const torch::Tensor& tensor,
                              int64_t start_dim = 0) {
    
    if (!tensor.is_contiguous())
      // TODO(heiner): Fix this non-contiguous case.
      throw py::value_error("Cannot convert non-contiguous tensor.");

    array->set_dtype(aten_to_dtype(tensor.scalar_type())); // TODO

    at::IntArrayRef shape = tensor.sizes();

    for (size_t i = start_dim, ndim = shape.size(); i < ndim; ++i) {
      array->add_shape(shape[i]);
    }

    // TODO: Consider set_allocated_data.
    // TODO: Consider [ctype = STRING_VIEW] in proto file.
    array->set_data(tensor.data_ptr(), tensor.nbytes());
  }

  // Copied from the private torch/csrc/utils/tensor_numpy.cpp.
  // TODO(heiner): Expose this in PyTorch then use that function.
  static int aten_to_dtype(const at::ScalarType scalar_type) {
    switch (scalar_type) {
      case at::kDouble:
        return NPY_DOUBLE;
      case at::kFloat:
        return NPY_FLOAT;
      case at::kHalf:
        return NPY_HALF;
      case at::kLong:
        return NPY_INT64;
      case at::kInt:
        return NPY_INT32;
      case at::kShort:
        return NPY_INT16;
      case at::kChar:
        return NPY_INT8;
      case at::kByte:
        return NPY_UINT8;
      case at::kBool:
        return NPY_BOOL;
      default: {
        std::string what = "Got unsupported ScalarType ";
        throw py::value_error(what + at::toString(scalar_type));
      }
    }
  }